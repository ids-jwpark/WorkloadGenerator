{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "from provider import *\n",
    "\n",
    "def bfloat16arr_to_uint8_arr(nparr, endian='little'):\n",
    "    # Directly convert bfloat16 to uint8 since numpy does not support bfloat16\n",
    "    if nparr.dtype != torch.bfloat16:\n",
    "        raise ValueError('bfloat16_to_uint8: nparr must be of type bfloat16')\n",
    "    if nparr.ndim != 1:\n",
    "        nparr = nparr.reshape(-1)\n",
    "    uint8_list = []\n",
    "    # pdb.set_trace()\n",
    "    # Since bfloat16 does not offer bitwise operation, we need to convert to int16 first\n",
    "    nparr = nparr.view(torch.int16) # int32로\n",
    "    for elem in nparr:\n",
    "        if endian == 'little':\n",
    "            uint8_list.append(elem & 0xFF)\n",
    "            uint8_list.append((elem >> 8) & 0xFF)\n",
    "\n",
    "        elif endian == 'big':\n",
    "            uint8_list.append((elem >> 8) & 0xFF)\n",
    "            uint8_list.append(elem & 0xFF)\n",
    "        else:\n",
    "            raise ValueError('bfloat16arr_to_uint8: endian must be either little or big')\n",
    "    nparr = np.array(uint8_list, dtype=np.uint8)\n",
    "    return nparr\n",
    "\n",
    "def fp16arr_to_uint8_arr(nparr, endian='little'):\n",
    "    # Directly convert bfloat16 to uint8 since numpy does not support bfloat16\n",
    "    \n",
    "    if nparr.ndim != 1:\n",
    "        nparr = nparr.reshape(-1)\n",
    "    uint8_list = []\n",
    "    # pdb.set_trace()\n",
    "    # Since bfloat16 does not offer bitwise operation, we need to convert to int16 first\n",
    "    nparr = nparr.view(torch.int16) # int32로\n",
    "    for elem in nparr:\n",
    "        if endian == 'little':\n",
    "            uint8_list.append(elem & 0xFF)\n",
    "            uint8_list.append((elem >> 8) & 0xFF)\n",
    "\n",
    "        elif endian == 'big':\n",
    "            uint8_list.append((elem >> 8) & 0xFF)\n",
    "            uint8_list.append(elem & 0xFF)\n",
    "        else:\n",
    "            raise ValueError('bfloat16arr_to_uint8: endian must be either little or big')\n",
    "    nparr = np.array(uint8_list, dtype=np.uint8)\n",
    "    return nparr\n",
    "\n",
    "# Change the function above to convert float32 array to uint8 arr\n",
    "def fp32arr_to_uint8_arr(nparr, endian='little'):\n",
    "    if nparr.dtype != torch.float32:\n",
    "        raise ValueError('fp32arr_to_uint8_arr: nparr must be of type float32')\n",
    "    if nparr.ndim != 1:\n",
    "        nparr = nparr.reshape(-1)\n",
    "    uint8_list = []\n",
    "    pdb.set_trace()\n",
    "    # Convert float32 to uint8 using numpy\n",
    "    # nparr = (nparr * 255).astype(np.uint8)\n",
    "    nparr = (nparr).view(torch.int32)\n",
    "    for elem in nparr:\n",
    "        if endian == 'little':\n",
    "            uint8_list.append(elem & 0xFF)\n",
    "            uint8_list.append((elem >> 8) & 0xFF)\n",
    "            # Add more since this is float32 (has 4 bytes)\n",
    "            uint8_list.append((elem >> 16) & 0xFF)\n",
    "            uint8_list.append((elem >> 24) & 0xFF)\n",
    "        else:\n",
    "            uint8_list.append((elem >> 24) & 0xFF)\n",
    "            uint8_list.append((elem >> 16) & 0xFF)\n",
    "            uint8_list.append((elem >> 8) & 0xFF)\n",
    "            uint8_list.append(elem & 0xFF)\n",
    "            \n",
    "    nparr = np.array(uint8_list, dtype=np.uint8)\n",
    "    return nparr\n",
    "\n",
    "def int4_to_uint16_array(nparr):\n",
    "    if nparr.dtype != torch.int8:\n",
    "        raise ValueError('packed_int4_to_uint16_array: nparr must be of type int8')\n",
    "    assert nparr.ndim == 2\n",
    "    assert nparr.shape[1] == 4, \"Only shapes (N, 4) are supported\"\n",
    "    assert nparr.max() < 8, nparr.min() > -9\n",
    "    uint16_list = []\n",
    "    for elem in nparr:\n",
    "        uint16 = 0\n",
    "        for i in range(4):\n",
    "            uint16 |= (elem[i].item() & 0xF) << (i * 4)\n",
    "        uint16_list.append(uint16)\n",
    "    \n",
    "    nparr = np.array(uint16_list, dtype=np.uint16)\n",
    "    return nparr\n",
    "\n",
    "def int4_to_uint8_array(nparr):\n",
    "    if nparr.dtype != torch.int8:\n",
    "        raise ValueError('packed_int4_to_uint16_array: nparr must be of type int8')\n",
    "    assert nparr.ndim == 2\n",
    "    assert nparr.shape[1] == 4, \"Only shapes (N, 4) are supported\"\n",
    "    assert nparr.max() < 8, nparr.min() > -9\n",
    "    uint8_list = []\n",
    "    for elem in nparr:\n",
    "        uint16 = 0\n",
    "        for i in range(4):\n",
    "            uint16 |= (elem[i].item() & 0xF) << (i * 4)\n",
    "        uint8_list.append(uint16 & 0xFF)\n",
    "        uint8_list.append((uint16 >> 8) & 0xFF)\n",
    "    \n",
    "    nparr = np.array(uint8_list, dtype=np.uint8)\n",
    "    return nparr\n",
    "\n",
    "def twos_complement_int4_to_binary(nparr):\n",
    "    if nparr.dtype != torch.int8:\n",
    "        raise ValueError('twos_complement_int4_to_binary: nparr must be of type int8')\n",
    "    binary_list = []\n",
    "    for elem in nparr:\n",
    "        # Each elenmement is 4 bits, but represented as 8 bits\n",
    "        # We need to convert to 4 bits\n",
    "        elem = elem & 0xF # But this is for unsigned int\n",
    "        # Convert to binary\n",
    "        binary = bin(elem)[2:][-4:]\n",
    "        # Also get sign\n",
    "        sign = 1 if elem < 0 else 0\n",
    "        # Add sign bit\n",
    "        binary = str(sign) + binary\n",
    "        # Add padding\n",
    "        binary_list.append(binary.zfill(4))\n",
    "    return binary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_load = torch.load(\"test_data.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79692/1916696458.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_i = torch.tensor(w, dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "x = data_load['in_A'].bfloat16()\n",
    "w = data_load['in_B'].bfloat16()\n",
    "result_ref = x @ w\n",
    "\n",
    "result_uint8 = bfloat16arr_to_uint8_arr(result_ref)\n",
    "result_uint16 = result_uint8.reshape(-1,2).view(np.uint16)\n",
    "with open('data_bf_int/dout_result.hex', 'w') as f:\n",
    "    for row in result_uint16:\n",
    "        f.write(' '.join(format(val, 'd') for val in row) + '\\n')\n",
    "\n",
    "#x_f = x.float().detach()\n",
    "#x_np = x_f.numpy()\n",
    "x_uint8 = bfloat16arr_to_uint8_arr(x)\n",
    "x_uint16 = x_uint8.reshape(-1, 2).view(np.uint16)\n",
    "\n",
    "with open('data_bf_int/din_a.hex', 'w') as f:\n",
    "    for row in x_uint16:\n",
    "        f.write(' '.join(format(val, 'X').zfill(4) for val in row) + '\\n')\n",
    "\n",
    "w_i = torch.tensor(w, dtype=torch.int8)\n",
    "w_uint8 = int4_to_uint8_array(w_i)\n",
    "w_uint16 = w_uint8.reshape(-1, 2).view(np.uint16)\n",
    "\n",
    "with open('data_bf_int/din_b.hex', 'w') as f:\n",
    "    for row in w_uint16:\n",
    "        f.write(' '.join(format(val, 'X').zfill(4) for val in row) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([48585], dtype=uint16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_uint8 = bfloat16arr_to_uint8_arr(x * 10)\n",
    "x_uint16 = x_uint8.reshape(-1, 2).view(np.uint16)\n",
    "xmaxidx = torch.argmax(x)\n",
    "x_uint16[xmaxidx-56]\n",
    "\n",
    "with open('data_bf_bf/din_a.hex', 'w') as f:\n",
    "    for row in x_uint16:\n",
    "        f.write(' '.join(format(val, 'X').zfill(4) for val in row) + '\\n')\n",
    "\n",
    "with open('data_bf_bf/din_b.hex', 'w') as f:\n",
    "    for row in w_uint16:\n",
    "        f.write(' '.join(format(val, 'X').zfill(4) for val in row) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(100,32).bfloat16()\n",
    "w = torch.randn(100,32).bfloat16()\n",
    "\n",
    "x_uint8 = bfloat16arr_to_uint8_arr(x)\n",
    "x_uint16 = x_uint8.reshape(-1, 2).view(np.uint16)\n",
    "\n",
    "w_uint8 = bfloat16arr_to_uint8_arr(w)\n",
    "w_uint16 = w_uint8.reshape(-1, 2).view(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3672,  0.0728, -1.5625,  ...,  0.1855, -1.2266, -1.1641],\n",
       "        [-0.0212,  0.3438,  0.4277,  ...,  1.2422, -0.2539, -1.2109],\n",
       "        [ 1.3594,  0.7617, -0.9570,  ...,  0.3281,  0.0237, -1.3125],\n",
       "        ...,\n",
       "        [-0.8242,  1.0781,  0.3105,  ..., -0.1650,  0.0320,  1.0000],\n",
       "        [-1.6328,  0.2695, -0.7148,  ...,  1.1562, -0.6914, -0.8867],\n",
       "        [ 0.8008,  1.1016, -0.8828,  ..., -0.0903,  0.2168,  1.5781]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_72394/1251581419.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_i = torch.tensor(w, dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "x = data_load['in_A'].half()\n",
    "w = data_load['in_B'].half()\n",
    "result_ref = x @ w\n",
    "\n",
    "result_uint8 = fp16arr_to_uint8_arr(result_ref)\n",
    "result_uint16 = result_uint8.reshape(-1,2).view(np.uint16)\n",
    "with open('data_fp_int/dout_result.hex', 'w') as f:\n",
    "    for row in result_uint16:\n",
    "        f.write(' '.join(format(val, 'd') for val in row) + '\\n')\n",
    "\n",
    "#x_f = x.float().detach()\n",
    "#x_np = x_f.numpy()\n",
    "x_uint8 = fp16arr_to_uint8_arr(x)\n",
    "x_uint16 = x_uint8.reshape(-1, 2).view(np.uint16)\n",
    "\n",
    "with open('data_fp_int/din_a.hex', 'w') as f:\n",
    "    for row in x_uint16:\n",
    "        f.write(' '.join(format(val, 'X').zfill(4) for val in row) + '\\n')\n",
    "\n",
    "w_i = torch.tensor(w, dtype=torch.int8)\n",
    "w_uint8 = int4_to_uint8_array(w_i)\n",
    "w_uint16 = w_uint8.reshape(-1, 2).view(np.uint16)\n",
    "\n",
    "with open('data_fp_int/din_b.hex', 'w') as f:\n",
    "    for row in w_uint16:\n",
    "        f.write(' '.join(format(val, 'X').zfill(4) for val in row) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_72394/4038822124.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_i = torch.tensor(w, dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "w_i = torch.tensor(w, dtype=torch.int8)\n",
    "w_uint8 = int4_to_uint8_array(w_i)\n",
    "w_uint16 = w_uint8.reshape(-1, 2).view(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 1)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int4_to_uint16_array(w_i).reshape(-1, 1).shape\n",
    "w_uint16.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_load['in_A'].half()\n",
    "w = data_load['in_B'].half()\n",
    "result_ref = x @ w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_f = x.float().detach()\n",
    "x_np = x_f.numpy()\n",
    "x_uint8 = fp16arr_to_uint8_arr(x_f)\n",
    "x_uint16 = x_uint8.reshape(-1, 2).view(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_half/din_a.hex', 'w') as f:\n",
    "    for row in x_uint16:\n",
    "        f.write(' '.join(format(val, 'X').zfill(4) for val in row) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0000'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format(0, 'x').zfill(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_uint8 = fp16arr_to_uint8_arr(result_ref)\n",
    "result_uint16 = result_uint8.reshape(-1,2).view(np.uint16)\n",
    "with open('data_half/dout_result.hex', 'w') as f:\n",
    "    for row in result_uint16:\n",
    "        f.write(' '.join(format(val, 'd') for val in row) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_half/din_b.hex', 'w') as f:\n",
    "    for row in w_uint16:\n",
    "        f.write(' '.join(format(val, 'X').zfill(4) for val in row) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "elem = w_i[6]\n",
    "uint16 = 0\n",
    "for i in range(4):\n",
    "    uint16 |= (elem[i].item() & 0xF) << (i * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 0], dtype=torch.int8)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hex(uint16)\n",
    "w_i[6]\n",
    "#bin(elem[3]&0xF) + bin(elem[2]&0xF)[2:] + bin(elem[1]&0xF)[2:] + bin(elem[0]&0xF)[2:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DWG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
